# mini project 10 - PySpark Data Processing

This repository contains a PySpark script for data processing using Apache Spark. In this project, we demonstrate how to perform data processing on a large dataset using PySpark, including the use of Spark SQL queries and data transformations.

## Requirements

- Python with PySpark installed
- A large dataset (e.g., in CSV format)

## Project Structure

- **main.py**: The PySpark script for data processing.
- **info.csv**: Sample input data (replace with your own dataset).
- **output.csv**: Output data generated by the script.

## Usage

1. Make sure you have Python and PySpark installed in your environment.

2. Place your dataset in the same directory as `main.py`, or update the file path in the script to point to your dataset.

3. Run the script by executing the following command:

   ```bash
   python main.py
   ```

4. The script will perform the following steps:
   - Initialize a Spark session.
   - Load the dataset from `info.csv`.
   - Rename the "title" column to "Game Title."
   - Create a temporary table "data_table."
   - Execute a Spark SQL query to select rows with a positive ratio of 100.
   - Write the result to a CSV file named "output.csv."

5. Once the script completes execution, you will find the processed data in the "output.csv" file.

## Output

When the sql runs:

<img width="682" alt="Screenshot 2023-11-12 at 14 18 17" src="https://github.com/nogibjj/Owner-avatar-mini-proj10-rc/assets/123079408/74849c57-b7d1-4415-bbd7-91dda9734381">

The output will be:


<img width="822" alt="Screenshot 2023-11-12 at 15 20 21" src="https://github.com/nogibjj/Owner-avatar-mini-proj10-rc/assets/123079408/4e62e9fe-a139-4251-bfef-9f14f9152959">


Please check output.csv file.
